
# Reviewer Dp35 — Rebuttal

**We sincerely thank the reviewer for the positive evaluation of our soundness, presentation quality, contributions, and motivation.**  
Your comments helped us clarify training‑cost comparisons and the nature of the gradient‑magnitude observation.

## 1. Comparison of Finetuning Costs

We agree that training cost is an important comparison point. For depth‑pruning methods, the main baseline **NOSE** adopts the same **iterative pruning + 400‑epoch finetuning** schedule. Our method follows exactly this protocol to ensure a fair comparison.  

**Therefore, our pruning pipeline does not introduce additional training cost beyond existing depth‑pruning baselines.**  

## 2. Why Gradient Disparity Appears

Thank you for highlighting the need to clarify this point. We observe the gradient disparity on the added importance parameters because these scalar gates **aggregate the full activation energy and all back‑propagated gradients of each branch into a single value**, making the contrast between Attention and FFN directly measurable.  

Importantly, as shown in our theoretical analysis **[Theorem 4 — Theoretical Analysis of Gradient Disparity in DeiT](https://anonconf2025.github.io/MathProof/prof4.html)**, the disparity itself *is a structural property* of ViTs:

• the FFN/GELU branch is **4× larger** in dimensionality, and  
• GELU activations exhibit **much larger second‑moment energy** due to heavy‑tailed outliers.

Consequently, the expected gradient magnitude on the GELU‑branch gate is approximately **4·γ times larger**, where **γ ≫ 1**. Thus, the added parameters do **not** create the disparity — they simply serve as a **clean lens** that reveals an intrinsic imbalance already present in the architecture.

----
Finally, we appreciate the reviewer’s constructive comments and will incorporate a discussion of limitations and training cost of baseline into the revised version.