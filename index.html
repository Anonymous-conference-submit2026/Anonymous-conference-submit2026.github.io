<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>BoundaryDPT: Project Overview</title>

<style>
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
        margin: 0;
        padding: 0;
        color: #222;
        background: #fafafa;
        line-height: 1.6;
    }

    header {
        background: #111;
        color: white;
        padding: 40px 20px;
        text-align: center;
    }

    header h1 {
        margin: 0;
        font-size: 2rem;
        font-weight: 700;
    }

    section {
        padding: 30px 20px;
        max-width: 900px;
        margin: auto;
    }

    h2 {
        border-left: 4px solid #0070f3;
        padding-left: 10px;
        margin-top: 40px;
        font-size: 1.6rem;
        display: flex;
        align-items: center;
        gap: 8px;
    }

    h2 .icon {
        font-size: 1.5rem;
    }

    .img-block {
        margin: 20px 0;
        text-align: center;
    }

    img {
        width: 100%;
        max-width: 700px;
        border-radius: 6px;
        background: white;
        box-shadow: 0 0 10px rgba(0,0,0,0.05);
    }

    footer {
        margin-top: 50px;
        padding: 20px;
        text-align: center;
        color: #666;
    }

    @media (max-width: 600px) {
        h1 { font-size: 1.7rem; }
        h2 { font-size: 1.3rem; }
    }
</style>

</head>

<body>

<header>
    <h1>BoundaryDPT: Pushing the Boundaries of Depth Pruning for Vision Transformers</h1>
</header>

<!-- ===================== 1. OVERVIEW ===================== -->
<section id="overview">
    <h2><span class="icon">üìò</span>1. Overview</h2>
    <p>
        BoundaryDPT is a new depth pruning framework for Vision Transformers (ViTs). It jointly prunes 
        self-attention layers and activation function layers while avoiding the dimension mismatch 
        problem that traditionally makes depth pruning unstable and inaccurate.
    </p>

    <div class="img-block">
        <img src="fig/Pareto.svg" alt="Pareto Frontier" />
    </div>
</section>

<!-- ===================== 2. MOTIVATION ===================== -->
<section id="motivation">
    <h2><span class="icon">üéØ</span>2. Motivation</h2>
    <p>
        Depth pruning provides significantly higher speedup than width pruning. However, previous 
        depth pruning techniques often cause severe accuracy drops. Our goal is to understand the 
        structural challenges inside ViTs and design a method that fully unlocks the potential of 
        depth pruning while maintaining accuracy.
    </p>

    <div class="img-block">
        <img src="fig/depth_vs_width_pruning.svg" alt="Depth vs Width" />
    </div>

    <div class="img-block">
        <img src="fig/dim_mismatch.svg" alt="Dimension Mismatch" />
    </div>

    <div class="img-block">
        <img src="fig/vit_latency.svg" alt="Layer Latency Distribution" />
    </div>
</section>

<!-- ===================== 3. KEY OBSERVATIONS ===================== -->
<section id="key-observations">
    <h2><span class="icon">üîç</span>3. Key Observations</h2>

    <p>
        Our analysis reveals two hidden but fundamental challenges inside ViTs:
    </p>

    <h3>‚Ä¢ Gradient Disparity</h3>
    <p>
        Activation layers and attention layers have very different gradient magnitudes, which causes 
        gradient-based pruning methods to be heavily biased.
    </p>

    <div class="img-block">
        <img src="fig/Gradient_Disparity.svg" alt="Gradient Disparity" />
    </div>

    <h3>‚Ä¢ Recovery Asymmetry</h3>
    <p>
        Activation-layer pruning initially produces large accuracy drops but recovers extremely fast. 
        Attention-layer pruning behaves in the opposite way.
    </p>

    <div class="img-block">
        <img src="fig/Recovery_Asymmetry.svg" alt="Recovery Asymmetry" />
    </div>

</section>

<!-- ===================== 4. METHOD ===================== -->
<section id="method">
    <h2><span class="icon">üß©</span>4. Method: BoundaryDPT</h2>

    <p>
        BoundaryDPT is a three-stage depth pruning framework that addresses gradient disparity, 
        recovery asymmetry, and dimension mismatch. The pipeline consists of:
    </p>

    <ul>
        <li><b>MAP-based pruning ratio determination</b> ‚Äî predicts accuracy for different combinations of pruned attention/activation layers.</li>
        <li><b>Training-based redundant layer identification</b> ‚Äî avoids biased importance ranking across different layer types.</li>
        <li><b>Activation-layer removal + MLP merging</b> ‚Äî safely reduces depth without breaking tensor shapes.</li>
    </ul>

    <div class="img-block">
        <img src="fig/overview.svg" alt="Method Overview" />
    </div>

    <div class="img-block">
        <img src="fig/pipeline.svg" alt="Pipeline" />
    </div>

</section>

<!-- ===================== 5. EXPERIMENTAL RESULTS ===================== -->
<section id="results">
    <h2><span class="icon">üìà</span>5. Experimental Results</h2>

    <p>
        BoundaryDPT achieves state-of-the-art depth pruning results on DeiT-S and DeiT-B, and when 
        combined with width pruning (BoundaryDPT+), it sets new benchmarks for extreme ViT compression.
    </p>

    <div class="img-block">
        <img src="fig/speedup_accuracy_comparison_combined.svg" alt="Speedup vs Accuracy Combined" />
    </div>

    <div class="img-block">
        <img src="fig/speedup_acc_curve_small.svg" alt="Speedup Accuracy Curve Small" />
    </div>

    <div class="img-block">
        <img src="fig/speedup_acc_curve_base.svg" alt="Speedup Accuracy Curve Base" />
    </div>

    <p>
        BoundaryDPT consistently improves the robustness and efficiency of ViT depth pruning, 
        achieving Pareto-optimal tradeoffs between accuracy and speed.
    </p>

</section>

<footer>
    ¬© 2025 BoundaryDPT Project
</footer>

</body>
</html>