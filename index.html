<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>BoundaryDPT: Project Overview</title>

<style>
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
        margin: 0;
        padding: 0;
        color: #222;
        background: #fafafa;
        line-height: 1.6;
    }

    header {
        background: #111;
        color: white;
        padding: 40px 20px;
        text-align: center;
    }

    header h1 {
        margin: 0;
        font-size: 2rem;
        font-weight: 700;
    }

    section {
        padding: 30px 20px;
        max-width: 900px;
        margin: auto;
    }

    h2 {
        border-left: 4px solid #0070f3;
        padding-left: 10px;
        margin-top: 40px;
        font-size: 1.6rem;
    }

    .img-block {
        margin: 20px 0;
        text-align: center;
    }

    img {
        width: 100%;
        max-width: 700px;
        border-radius: 6px;
        background: white;
        box-shadow: 0 0 10px rgba(0,0,0,0.05);
    }

    footer {
        margin-top: 50px;
        padding: 20px;
        text-align: center;
        color: #666;
    }

    @media (max-width: 600px) {
        h1 { font-size: 1.7rem; }
        h2 { font-size: 1.3rem; }
    }
</style>

</head>

<body>

<header>
    <h1>BoundaryDPT: Pushing the Boundaries of Depth Pruning for Vision Transformers</h1>
</header>

<section>
    <h2>ðŸŒŸ Key Observations</h2>
    <p>
        BoundaryDPT is motivated by two core phenomena we discovered inside Vision Transformers:
    </p>

    <ul>
        <li><b>Gradient Disparity</b>: Activation layers and attention layers exhibit significantly different gradient magnitudes, causing biased importance estimation.</li>
        <li><b>Recovery Asymmetry</b>: Activation-layer pruning initially destroys accuracy more severely, yet recovers much faster during fine-tuning than attention-layer pruning.</li>
        <li><b>Dimension Mismatch Problem</b>: Joint depth pruning of attention + MLP linear layers causes tensor shape inconsistency, preventing naÃ¯ve pruning from working.</li>
    </ul>

    <div class="img-block">
        <img src="fig/Pareto.svg" alt="Pareto Frontier" />
    </div>

    <div class="img-block">
        <img src="fig/Gradient_Disparity.svg" alt="Gradient Disparity" />
    </div>

    <div class="img-block">
        <img src="fig/Recovery_Asymmetry.svg" alt="Recovery Asymmetry" />
    </div>

    <div class="img-block">
        <img src="fig/depth_vs_width_pruning.svg" alt="Depth vs Width" />
    </div>

    <div class="img-block">
        <img src="fig/dim_mismatch.svg" alt="Dimension Mismatch" />
    </div>

    <div class="img-block">
        <img src="fig/vit_latency.svg" alt="Layer Latency Distribution" />
    </div>
</section>

<section>
    <h2>ðŸš€ Method: BoundaryDPT</h2>
    <p>
        BoundaryDPT is a threeâ€‘stage depth pruning framework that jointly prunes selfâ€‘attention layers
        and activation function layersâ€”while resolving the dimension mismatch problem. 
        We introduce:
    </p>

    <ul>
        <li><b>MAP-based pruning ratio determination</b> â€” modeling ViT accuracy drop via a polynomial predictor.</li>
        <li><b>Training-based redundant layer identification</b> â€” eliminating cross-type bias caused by gradient disparity.</li>
        <li><b>MLP merging mechanism</b> â€” removing activation layers to allow safe merging of linear layers, reducing ViT depth.</li>
    </ul>

    <div class="img-block">
        <img src="fig/overview.svg" alt="Method Overview" />
    </div>

    <div class="img-block">
        <img src="fig/pipeline.svg" alt="Pipeline" />
    </div>
</section>

<section>
    <h2>ðŸ“ˆ Experimental Results</h2>
    <p>
       BoundaryDPT achieves **state-of-the-art depth pruning** on DeiT-S/B and outperforms both
       width pruning and previous depth pruning methods. Combined with width pruning
       (BoundaryDPT+), it sets new records in extreme ViT compression.
    </p>

    <div class="img-block">
        <img src="fig/speedup_accuracy_comparison_combined.svg" alt="Speedup vs Accuracy Combined" />
    </div>

    <div class="img-block">
        <img src="fig/speedup_acc_curve_small.svg" alt="Speedup Accuracy Curve Small" />
    </div>

    <div class="img-block">
        <img src="fig/speedup_acc_curve_base.svg" alt="Speedup Accuracy Curve Base" />
    </div>

    <p>
        BoundaryDPT improves depth pruning robustness and enables new Paretoâ€‘optimal
        accuracyâ€‘speed tradeoffs for Vision Transformers.
    </p>
</section>

<footer>
    Â© 2025 BoundaryDPT Project
</footer>

</body>
</html>